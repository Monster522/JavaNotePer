1. 怎么做到增加机器能线性增加性能的?
线性扩容有两种情况，一种是“无状态”服务，比如常见的 web 后端；另一种是“有状态服务”，比如 mysql 这种数据库。

对于无状态服务，一般只要解决了“服务发现”功能，在服务启动之后能够把请求量引到新服务上，就可以做到线性扩容。常见的服务发现包括 DNS 调度（通过域名解析到不同机器）、负载均衡调度（通过反向代理服务转发到不同机器）或者动态服务发现（比如 dubbo ），等等。

对于有状态服务，除了要解决服务发现问题之外，还要解决状态（数据）迁移问题，迁移又分两步：先是数据拆分，常见的都是用哈希或者一致性哈希把数据打散。然后是迁移，常见的办法有快照和日志两类迁移方式。也有一些数据库直接实现了开发无感知的状态迁移功能，比如 hbase。

2. 代码优化，从哪些方向入手?
初期的优化主要集中在功能上，不写 bug。
然后是鲁棒性，在异常情况下不写 bug。
然后是性能，提高系统吞吐量，或者执行降低延迟。
然后是可维护性，在团队开发过程中降低其他人的理解难度，再做好一些，通过设计做到模块解耦，降低删除无用代码的难度。
然后是可扩展性，能够预测系统或者业务的发展方向，提前设计好锚点，让系统能够通过扩展而不是修改的方式增加功能。

3. 做「你关注的某某人评论了某某人」之类的和几度人际关系相关的复杂功能的时候有没有遇到哪些性能上可能的问题?
这肯定是有性能挑战的，比如“你关注的人也在关注”、“你关注的人最近发的微博”等等，都可以理解是二度关系。主要挑战点是数据的扇出量会比较大，我关注了 1000 人，这 1000 人又每人关注了 1000 人，那就是要 100 万的数据做处理。解决办法要么是减少扇出（比如限制关注人数量），要么是离线算数据，在线取结果。

4. 视频存储和提供有什么难点嘛，常年 mysql 中 varchar 选手想了解下?
视频存储和播放难点要分开说。

对于分布式文件系统来说，有几个难点。
一个是文件大带来的执行效率，比如用户上传一个 10G 的文件，要 1 秒之后立刻能够访问，需要做一些性能优化的策略
一个是可用性，在某台机器宕机之后不能影响用户数据，需要有数据迁移和冗余的策略。
一个是文件多带来的元信息膨胀，分布式文件系统都要保存每个 key 的元信息（比如存在哪台机器上），当文件超过几百亿之后也会带来元信息存取的压力。

而播放一方面是整体缓存架构和调度策略的选择，另一方面的难点主要是对于网络（在目前场景下，主要是 tcp 协议）的理解、对协议（比如 http/http over quic ）的理解和策略的选择。

当然，在国内 isp 环境下，更多的还是与人斗，其乐无穷。

5. 分布式事务这块是怎么处理的呢？是由业务系统去做一致，幂等之类的保证吗？还是有统一的中间件或框架呢?
微博这边的一致性要求并不高，一般是通过幂等性和常规的乐观、悲观锁实现的，分布式事务（至少在我这里）用的不多。

6. 视频类的后端开发和其他图文为主的社区产品后端开发，架构、技术选型上有哪些不一样的地方?
从整体的角度来抽象看，要做的东西其实差不多，都会有增删改查，然后内容理解+推荐；视频特殊一些的地方是有视频编解码。
具体技术选型来说的话，业务上的增删改查都差不多，但是视频存储都是对象存储服务而非关系型数据库；
视频方向的内容理解更多的偏向深度学习的实现；
视频编解码是一门独立的专业，不过由于太耗计算资源所以还要配合着调度系统一起实现。

7. 楼主有参与用 serverless 搭建后台系统吗？或建立部分 serverless 服务?
我这里没有作为使用方实践过，反而我们团队做的是更像是实现 serverless 的基础设施。不过据我了解微博内有一些团队已经有过实践了，具体效果我也不太清楚……

8. 可以透露一下新浪短链接的一些思路么?
就是个 key-value 的映射关系，感觉网上这类服务实现思路和开源代码一搜一大把啊……

9. 怎么做才能做到不同地区，用户播放视频都比较流畅？会用到类似 CDN 的部署架构?
如果往深了说，流畅是几方面的综合结果，包括视频体积、CDN 部署、播放调度、防劫持、播放调度、防劫持等等。

对于你说的不同地区来说，最重要的方面有两个：一个是 CDN 部署和调度情况，尽量让用户访问边缘节点；然后是防止劫持，一般流量被劫持后都不可避免的性能变差……

10. 应用服务器的数据库连接池应该设多大？看了一篇译文 https://www.jianshu.com/p/a8f653fc0c54 的观点是"连接数 = ((核心数 * 2) + 有效磁盘数)", 不知道实际一般是不是这样计算?
我没太看懂你的问题，到底是数据库服务的连接池，还是应用服务连接数据库的连接池？文章里我简单扫了一眼，似乎是后者

不过无论哪个连接池，核心问题还是“同一时间内，需要同时请求的数量”，这个其实就是个数学公式，类似“这条路上每天要跑 1000 辆车，每辆车跑个来回要 10 分钟，那么路建多宽合适”。按我的经验，连接数多设一点不会有太多问题（除非设的数量太夸张把系统连接数耗尽了），而设少了，在系统负载变高的时候就会出现非常明显的排队现象，这对服务性能的影响更大一些。

11. 在系统或功能模块设计阶段是如何考虑系统的扩展性的呢？是快速原型，实现，上线，后续迭代升级，还是说会在一开始就做一些复杂的设计？在这方面是怎么作取舍呢？
设计阶段要考虑的首先是“系统哪些功能是必不可少并且需要快速验证的”，然后是“系统 2 年以内有可能会有什么变化”，觉着不好设计的原因还是设计少了，踩的坑不够多。经验多了，就没这类问题了。

12. 一个人有几千万个关注，这种数据结构怎么存呢。？
存不是问题，你用个普通 mysql，只要哈希的够多，都能存的了。难的问题是“怎么取”，这个要看业务场景。

13. 有使用 K8S 应用的案例吗，比如说用在数据库扩容上面？
我的团队应用 k8s 更多的是在线下或者半离线业务场景下，做后台系统或者一些非实时任务。用来降低整体计算成本。至于线上业务，k8s 的能力还不足以让微博这种体量的公司开箱即用，这个改造成本也会比较高。

14. 新浪微博的用户关系是怎么维护的，如果一个人的粉丝非常多，怎么快速的找出他的互粉好友?
我不是专业做关系服务的，只提供个思路：

1 不管如何取数据，在数据量大的情况下，查找问题都会变成两个具体问题：1 如何哈希，2 如何建索引。
2 一份数据可以有多份索引，针对不同的业务场景也可以有异构的索引存储。
3 在满足 1，2 的情况下，索引问题会变成“如何维护数据一致性”的问题，但是这个问题解决起来要简单很多。

15. 聊天记录是怎样存储在数据库中的？因为聊天记录肯定比关注列表更多更复杂，想知道后端是怎么设计的?
如果要设计一个存储聊天记录的系统，那只需要把标识聊天会话的 id 和对应的消息存起来就可以了吧，没理解具体难点是什么。

16. 处理高并发有哪些难点？以前感觉这块挺神秘的，接触不到?
能用缓存就用缓存
考虑并发场景下的一致性
在框架里做好断路器和保护机制
做压测和容量预估
加机器-.-

要做高并发，还是要有场景。这玩意就是个难者不会，会者不难的东西……

17. 在公司业务没有多大并发量的情况下，要自己模拟学习高并发有什么好的建议吗?
说实话，不太好搞。这问题就好像是“我现在身无分文，但是想今天就规划一下自己身价 10 亿之后的购物清单”一样，大部分情况下还是会受制于自己的经验。

我建议真要做就换个有高并发场景的平台。如果真的非要自己做，一方面可以看一些大厂技术大会的分享了解一下业务场景；一方面找一个现成的 app （比如微博），然后自己实现一个，自己压测看看极限在哪里。

18. 大佬你现在自身的技术栈是怎么样的,3 年后端参考下?

我是属于那种“什么都会一点”的技术专家，如果单独说技术栈的话就太多了，列不下来。

换个回答方向，说一下我认为自己的技术强项吧。

一个是系统设计能力，能够设计微博这种用户和流量规模的后端服务。
一个是对操作系统、网络和 VM 的理解，能够排查复杂性能问题。
一个是业务方面的能力，包括通讯直播视频和社交媒体相关业务和对应技术（消息推送、视频编码、文件存储，等等）

总结下来的话，就是基础知识+架构经验+领域知识吧。

19. 很多公司要求的大数据高并发经验，在小公司工作的机会永远用不到的如果才能通过这种 offer?
分两方面说，很多地方招聘（包括我这边）虽然更倾向于有高并发经验的人，但是这也不是个绝对的必选项。我的判断条件是“是否达到了所在平台的天花板”和“是否有持续进步的潜力”，大部分情况下，能把当前工作搞的很好的人，也能把高并发搞的很好。

另一方面，我一直认为能去大厂还是尽可能的去大厂，毕竟大厂能带来的经验和提升很多小公司确实没有。通过自学能储备一些知识，网上的教程也有不少，不过经验的差距很难解决。

简单来说，把自己想象成应届生，工作几年没有高并发经验和应届生没有工作经验面临的状况是类似的。

20. 一个合格的 5 年后端要具备哪些能力?
这问题有些宽泛，一线互联网公司的五年跟不知名公司的五年，完全不是一个概念。

我只说一线互联网公司的五年，基本上应该是小组长，能够独立设计日活百万级别的后端系统架构，在开发规范和效率方面能够指导初级工程师和工作。此外还有复杂问题的分析和解决能力

21. 如果想往架构师方向发展（不做普通码农），应该是什么样的学习路线？我之前大致都了解了前后端的很多技术，但都不精细。我前段时间才发现自己在数据一致性上面忽略了很多东西；前几天才发现有个叫 DDD （领域驱动设计）的东西，学了以下发现以前写的思路很丑陋。所以有什么学习路线？（或者能不能给一些专有名词）?
我觉着架构师主要是靠经验，其次才是学习。架构师的核心价值是在某个场景下构建合理的应用，如果没有架构的应用场景的话，那即使学了一堆名词，也只能硬套概念而不是解决问题，我觉着意义不大。但如果就是想了解这些名词的话，各种技术公众号或者微博关注一波，每天一大堆在上面扯名词的，有些名词我都不认识……

22. 请问大佬，如果想重试大数据开发的话，平时应该多关注哪些技术栈呢，从服务端开发转过去有哪些地方需要了解提升的?
大数据开发这个职位挺缥缈的……我不清楚你说的大数据开发是指的数据分析、数据中间件、有大数据场景的后端应用还是什么别的。

第一个，一般就是流式处理框架，Hadoop 全家桶之类的数据处理框架。
第二三个，跟普通后端开发没啥本质区别。

23. 用户的关注列表分页拉取时，如何把正在直播的主播前置到列表?
假设正在直播的主播不多，可以单独存正在直播的主播，插到关注列表前面。

24. 主播的粉丝量很大时，开播如何及时推送给粉丝?
1 想办法并行取粉丝关系，并行推送 2 优先推送核心用户 3 改成拉取模式，实时获取当前主播的直播状态

25. 怎么看待 RESTful 风格架构规范这东西.你所接触到的项目工程，达到 成熟度模型的几级?
我觉着这个东西比较理想化，实践过程里并没有过多的对接口做约束。因为我们是服务提供方，因此很多实践都依赖于调用发起方的能力。比如调用方说不支持 put，那就没法按 RESTful 风格写 put 接口。

26. 如何根据具体业务来优化 VM?
看 VM 在这个业务场景下的表现有什么问题，然后优化就可以了啊？
如何根据场景发现问题：模拟场景，测试
如何根据问题做优化：用工具分析问题→调整参数→验证是否解决→调参就是解决不了→查看源码进一步分析问题→修改源码→验证是否解决。

27. 应届生如何提高自己？看书总觉得是一个个点，学的技术很杂，前后端 /移动 app 都会一点，无法系统地整合?
多实践，找个有点难度的问题解决，实践一段时间再回过头来看书，会有不一样的收获。

28. 请问您如何看待之前的微服务和目前提倡的无服务，以及目前火热的中台的概念？这些概念的价值如何，背后是技术的演进，还是营销的泡沫？了解学习相关的东西，可以构建起自己的技术壁垒吗？
概念肯定是多了解一些更好，一个概念代表了对一类问题和解决方案的高层次的抽象。但是光靠概念这东西怎么可能构建出技术壁垒……

29. 请问一下，假设一个集群里 a,b,c 三个服务（或者叫应用）都依赖于 d （ v1.0 ）这个服务，现在需要把 d 这个服务，从 1.0 升级到 2.0?
目前我的操作流程是额外开一台机器用于跑 d （ v1.0 ），然后把 d 的所有机器都升级到 2.0，然后把 a，b，c 三个服务都升级到依赖于 d （ v2.0 ），再把 d （ v1.0 ）这台机器关闭。
感觉这么做很麻烦，而且也不自动。因为 a，b，c 都依赖于 1.0 版本，所以在整个升级过程，d 服务的两个版本，1.0 和 2.0 都是存在着的。请问能介绍一下你们的经验吗？我这里用的是 dubbo，每个服务各有两台机器运行着。

这是个通用问题，解决办法是保持接口向前兼容。可以理解是先上 D1.5 版，升级完 ABC，再升级到 D2.0

30.  后端扩容有什么好的方案和建议呢？数据库变大，需要动态增加机器，怎样一种方案可以无感知增加？
要么是提前做逻辑分表，容量瓶颈之后把逻辑表做物理迁移。或者干脆用 hbase 这种天然扩容无感知的数据库。

31. 数据量变大，查询效率如何维持？
提前做容量预估

32. 老数据的删除，我们项目上有，不知道大佬的项目中怎么处理？
对 mysql 来说就是按月分表，定期删除过期表。对 hbase 之类的设个 ttl





